{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\n",
    "def Prob(z):\n",
    "    P = np.zeros((2,2,2))\n",
    "    P[1,:,0] = 0.3\n",
    "    P[1,:,1] = 0.7\n",
    "    P[0,0,1] = z[1]*0.8+0.1\n",
    "    P[0,0,0] = 0.9- z[1]*0.8\n",
    "    P[0,1,1] = 0.3+z[1]*0.55\n",
    "    P[0,1,0] = 0.7-0.55* z[1]\n",
    "    return P\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self,W,mu,r,pi,k):\n",
    "\n",
    "        #W is the connection matrix\n",
    "        #K is the number of clusters\n",
    "        #r1 is the reward for infection\n",
    "        #r2 is the reward for wearing a mask\n",
    "        #mu is the initial state distribution (which is a matrix)\n",
    "        self.nstate = 2\n",
    "        self.naction = 2\n",
    "        self.pi = pi\n",
    "        self.mean_field = mu\n",
    "        self.w = W\n",
    "   \n",
    "\n",
    "        self.discount = 0.95\n",
    "        self.z = np.zeros((2,2))\n",
    "       \n",
    "        self.r = r\n",
    "        self.K = k\n",
    "        self.lam = 0.1\n",
    "        self.update_z()\n",
    "\n",
    "    def h_func(self,x):\n",
    "        return -(x*np.log(x)).sum()*self.lam\n",
    "        # this function is strongly convex corresponding to x\n",
    "\n",
    "\n",
    "    def reward(self,s,a,k):\n",
    "      return -self.r[k]*(s==1)-(a==0)-(a==1)*(s==0)*0.5\n",
    "      #this is the reward function for the kth population\n",
    "\n",
    "\n",
    "    # H = 0 ,S =1\n",
    "    # Y =0 , N =1\n",
    "    def update_z(self):\n",
    "      self.z = self.w @self.mean_field \n",
    "      self.z /= self.K\n",
    "      \n",
    "      \n",
    "    def transition(self,k):\n",
    "        z = self.z[k]\n",
    "        P = Prob(z)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def get_transition(self,k):\n",
    "      # this function is used to for the policy evaluation\n",
    "        transition = np.zeros((self.nstate,self.nstate))\n",
    "        for s in range(self.nstate):\n",
    "            for s1 in range(self.nstate):\n",
    "                transition[s,s1] = self.transition(k)[s,:,s1]@self.pi[k,s,:]\n",
    "        return transition\n",
    "\n",
    "\n",
    "    def Population_update(self):\n",
    "        #one step population update for the k clusters\n",
    "        #pi is a mixed policy here\n",
    "        ans = np.zeros((self.K,self.nstate))\n",
    "        for k in range(self.K):\n",
    "            P = self.transition(k)\n",
    "            for s in range(2):\n",
    "                for s1 in range(2):\n",
    "                    ans[k][s] += self.mean_field[k][s1]*self.pi[k][s1]@P[s1,:,s]\n",
    "        self.mean_field = ans.copy()\n",
    "        return ans\n",
    "      \n",
    "      \n",
    "    def pop_inf(self,iter =1000):\n",
    "      #this is the stabilized populaion distribution under the current policy\n",
    "        for i in range(iter):\n",
    "            self.Population_update()\n",
    "\n",
    "\n",
    "    def Vh_func(self,k):\n",
    "      #use policy evaluation to compute the regularized value function\n",
    "        P = self.get_transition(k)\n",
    "        reward = np.zeros((self.nstate))\n",
    "        for state in range(self.nstate):\n",
    "            #compute the expection of the reward\n",
    "            reward[state] += self.h_func(pi[k,state,:])\n",
    "            for action in range(self.naction):\n",
    "                reward[state] += self.reward(state, action,k)*self.pi[k,state,action]\n",
    "        A = np.zeros((self.nstate,self.nstate))\n",
    "        A = np.eye(self.nstate) - self.discount* P\n",
    "        ans = np.linalg.solve(A, reward)\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def Qh_func(self,s,a,k):\n",
    "      #use the regularized value function to compute the regularized q function\n",
    "        ans = self.reward(s, a, k)+self.h_func(self.pi[k,s,:])\n",
    "        V = self.Vh_func(k)\n",
    "        P = self.transition(k)\n",
    "        for s1 in range(2):\n",
    "            ans += self.discount* V[s1]*P[s,a,s1]\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def qh_func(self,s,a,k):\n",
    "        ans = self.Qh_func(s, a, k)\n",
    "        ans -= self.h_func(self.pi[k,s,:])\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def Gamma_q_func(self,k):\n",
    "        ans = np.zeros((2,2))\n",
    "        for s in range(self.nstate):\n",
    "            for a in range(self.naction):\n",
    "                ans[s,a] = self.Qh_func(s, a, k)\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def mirror(self,k,s,eta=0.1):\n",
    "      #one step policy mirror acent\n",
    "        q = self.Gamma_q_func(k)\n",
    "        pi = self.pi[k,:,:]\n",
    "        #mirror descent step\n",
    "        def func(u):\n",
    "            res = (u*q[s,0]+(1-u)*q[s,1]+(-u*np.log(u)-(1-u)*np.log(1-u))*self.lam)*2*eta-((u-pi[s,0])**2+((1-u)-pi[s,1])**2)\n",
    "            return res*-1\n",
    "        interval = (0,1)\n",
    "        ans = minimize_scalar(func, bounds=interval, method='bounded')\n",
    "        return np.array([ans.x,1-ans.x])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
