{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Initialize '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pathlib\n",
    "from scipy.stats import entropy\n",
    "import numpy as np \n",
    "from solver.policy.random_policy import RandomFinitePolicy\n",
    "from solver.stoc_reg_omd_graphon_solver import stoc_reg_DiscretizedGraphonExactOMDSolverFinite\n",
    "from solver.stoc_reg_omd_graphon_solver_2 import stoc_reg_DiscretizedGraphonExactOMDSolverFinite_2\n",
    "from solver.reg_omd_graphon_solver import reg_DiscretizedGraphonExactOMDSolverFinite\n",
    "from solver.omd_graphon_solver import DiscretizedGraphonExactOMDSolverFinite\n",
    "from solver.reg_graphon_solver import reg_DiscretizedGraphonExactSolverFinite\n",
    "from solver.graphon_solver import DiscretizedGraphonExactSolverFinite\n",
    "from evaluator.graphon_evaluator import DiscretizedGraphonEvaluatorFinite\n",
    "from evaluator.reg_graphon_evaluator import reg_DiscretizedGraphonEvaluatorFinite\n",
    "from evaluator.stochastic_evaluator import StochasticEvaluator\n",
    "from games.finite.beach import BeachGraphon\n",
    "from games.finite.cyber import CyberGraphon\n",
    "from games.finite.cyber_het import HeterogeneousCyberGraphon\n",
    "from games.finite.investment import InvestmentGraphon\n",
    "from games.finite.sis import SISGraphon\n",
    "from games.graphons import uniform_attachment_graphon, er_graphon, ranked_attachment_graphon, power_law_graphon, cutoff_power_law_graphon, sbm_graphon, exp_graphon\n",
    "from simulator.graphon_simulator import DiscretizedGraphonExactSimulatorFinite\n",
    "from simulator.stochastic_simulator import StochasticSimulator\n",
    "\"\"\" Initialize \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_graphon_1(x,y):\n",
    "    return 1\n",
    "def const_graphon_2(x,y):\n",
    "    return 0.5\n",
    "def const_graphon_3(x,y):\n",
    "    return 0\n",
    "\n",
    "regularization = 1 \n",
    "num_alphas = 30\n",
    "game = BeachGraphon(**{\"graphon\": exp_graphon})\n",
    "# game1 = BeachGraphon(**{\"graphon\": const_graphon_1})\n",
    "# game2 = BeachGraphon(**{\"graphon\": const_graphon_2})\n",
    "# game3 = BeachGraphon(**{\"graphon\": const_graphon_3})\n",
    "simulator = DiscretizedGraphonExactSimulatorFinite(**{})\n",
    "evaluator = reg_DiscretizedGraphonEvaluatorFinite(**{'regularization':regularization,'num_alphas':num_alphas})\n",
    "#solver = stoc_reg_DiscretizedGraphonExactOMDSolverFinite(**{'regularization':regularization, 'num_alphas': num_alphas, 'K': 10})\n",
    "solver1 = stoc_reg_DiscretizedGraphonExactOMDSolverFinite_2(**{'regularization':regularization, 'num_alphas': 5, 'K': 300})\n",
    "# solver2 = stoc_reg_DiscretizedGraphonExactOMDSolverFinite_2(**{'regularization':regularization, 'num_alphas': 10, 'K': 100})\n",
    "solver3 = reg_DiscretizedGraphonExactOMDSolverFinite(**{'regularization':regularization,'num_alphas':num_alphas})\n",
    "# solver4 = DiscretizedGraphonExactOMDSolverFinite(**{'num_alphas':num_alphas})\n",
    "\n",
    "eval_solver = reg_DiscretizedGraphonExactSolverFinite(**{'regularization':regularization,'num_alphas':num_alphas})\n",
    "iterations = 25\n",
    "\n",
    "big_logs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RandomFinitePolicy(game.agent_observation_space, game.agent_action_space)\n",
    "mu, info = simulator.simulate(game, policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# game.reward_g(t=0,x=[0.2,5],u=1,g=policy)\n",
    "mu.mu_alphas[100].mus[10][5]\n",
    "alphas  = mu.alphas\n",
    "a = 0.03\n",
    "index = np.argmin(abs(alphas-a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.mfg_wrapper import MFGGymWrapper\n",
    "data_cache = []\n",
    "env = MFGGymWrapper(game, mu, time_obs_augment=False)\n",
    "temp_policy = RandomFinitePolicy(game.agent_observation_space, game.agent_action_space)\n",
    "for alpha in solver1.alphas:\n",
    "    alpha_data_log = []\n",
    "    for _ in range(solver1.K):\n",
    "        temp_data_log = []\n",
    "        observation = env.reset()\n",
    "        observation = (alpha,observation[1])\n",
    "        done = 0\n",
    "        while not done:\n",
    "            action = temp_policy.act(env.t, observation)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            temp_data_log.append([observation,action,reward,next_observation])\n",
    "            observation = next_observation\n",
    "\n",
    "        alpha_data_log.append(temp_data_log)\n",
    "    data_cache.append(alpha_data_log)\n",
    "    #print(\"finish agent initialization\")\n",
    "\n",
    "Q_alphas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_idx,alpha in zip(range(len(solver1.alphas)), solver1.alphas):\n",
    "            Vs = []\n",
    "            Qs = []\n",
    "            curr_V = [0 for _ in range(game.agent_observation_space[1].n)]\n",
    "\n",
    "            for t in range(game.time_steps).__reversed__():\n",
    "                Q_t_pi = []\n",
    "                for x in range(game.agent_observation_space[1].n):\n",
    "                    x = tuple([alpha, x])\n",
    "\n",
    "                    Q_tx_pi = []\n",
    "                    temp_data = [data_cache[agent_idx][repeat][t] for repeat in range(solver1.K)]\n",
    "                    for u in range(game.agent_action_space.n):\n",
    "                        transit_est = []\n",
    "                        temp_samples = []\n",
    "                        for index in range(solver1.K):\n",
    "                            if temp_data[index][0][1]== x[1] and temp_data[index][1]== u:\n",
    "                                temp_samples.append(temp_data[index][3][1])\n",
    "                        #print(temp_samples[0])\n",
    "                        if len(temp_samples):\n",
    "                            for temp_S in range(game.agent_observation_space[1].n):\n",
    "                                temp_count = temp_samples.count(temp_S)/len(temp_samples)\n",
    "                                transit_est.append(temp_count)\n",
    "                        else:\n",
    "                            #print(alpha)\n",
    "                            #print(t)\n",
    "                            for temp_S in range(game.agent_observation_space[1].n):\n",
    "                                transit_est.append(1/game.agent_observation_space[1].n)\n",
    "                        #print(transit_est)\n",
    "                        temp_Q_est = game.reward(t, x, u, mu) + (1 - game.done(t, x, u, mu)) * np.vdot(curr_V, transit_est)\n",
    "                        Q_tx_pi.append(temp_Q_est)\n",
    "                    #           np.vdot(curr_V, mfg.transition_probs(t, x, u, mu))\n",
    "                    #Q_tx_pi = [mfg.reward(t, x, u, mu) + (1 - mfg.done(t, x, u, mu)) *\n",
    "                    #           np.vdot(curr_V, mfg.transition_probs(t, x, u, mu))\n",
    "                    #           for u in range(mfg.agent_action_space.n)]\n",
    "                    #Q_t_pi is a Q-function at time t \n",
    "                    Q_t_pi.append(Q_tx_pi)\n",
    "                curr_V = [np.vdot(Q_t_pi[x], policy.pmf(t, tuple([alpha, x])))+entropy(policy.pmf(t, tuple([alpha, x]))) for x in range(len(curr_V))]\n",
    "                Vs.append(curr_V)\n",
    "                Qs.append(Q_t_pi)\n",
    "            Vs.reverse()\n",
    "            Qs.reverse()\n",
    "            Q_alphas.append(Qs)\n",
    "if solver1.y is None:\n",
    "    solver1.y = solver1.omd_coeff2 * np.array(Q_alphas)\n",
    "else:\n",
    "    solver1.y = solver1.omd_coeff1*solver1.y + solver1.omd_coeff2 * np.array(Q_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07229388, 0.27398899, 0.65371713])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we can update the policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver.policy.graphon_policy import DiscretizedGraphonFeedbackPolicy\n",
    "from solver.policy.finite_policy import QMaxPolicy, QSoftMaxPolicy\n",
    "policy = DiscretizedGraphonFeedbackPolicy(game.agent_observation_space, game.agent_action_space,\n",
    "                                                  [\n",
    "                                                      QSoftMaxPolicy(game.agent_observation_space, game.agent_action_space, Qs, 1 / solver1.eta)\n",
    "                                                      for Qs, alpha in zip(solver1.y, solver1.alphas)\n",
    "                                                  ], solver1.alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.05, 0.9 , 0.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = game.transition_probability_matrix(t, alpha, policy,\n",
    "                                                DiscretizedGraphonMeanField(game.agent_observation_space,\n",
    "                                                                            [ConstantFiniteMeanField(\n",
    "                                                                                game.agent_observation_space,\n",
    "                                                                                mu_alpha)\n",
    "                                                                            for mu_alpha in mu_alphas_curr],\n",
    "                                                                            simulator.alphas))\n",
    "\n",
    "mu_new = DiscretizedGraphonMeanField(game.agent_observation_space,[ConstantFiniteMeanField(game.agent_observation_space,mu_alpha)for mu_alpha in mu_alphas_curr],simulator.alphas)\n",
    "\n",
    "\n",
    "np.sum([np.array([policy.pmf(t, tuple([alpha, x]))[u] * game.transition_probs(t, tuple([alpha, x]), u, mu_new)\n",
    "                                 for x in range(game.agent_observation_space[1].n)])\n",
    "                       for u in range(policy.action_space.n)], axis=0)\n",
    "\n",
    "[policy.pmf(t, tuple([alpha, x]))[u] * game.transition_probs(t, tuple([alpha, x]), u, mu_new)\n",
    "                                 for x in range(game.agent_observation_space[1].n)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator.mean_fields.graphon_mf import DiscretizedGraphonMeanField\n",
    "from simulator.mean_fields.finite_mf import ConstantFiniteMeanField, ExactFiniteMeanField\n",
    "mus = []\n",
    "mu_alphas_curr = []\n",
    "for alpha in simulator.alphas:\n",
    "    mu_alphas_curr.append(game.initial_state_distribution.dist2(alpha).probs.numpy())\n",
    "\n",
    "for t in range(game.time_steps):\n",
    "    mus.append(mu_alphas_curr)\n",
    "    mu_alphas_next = []\n",
    "    for idx, alpha in zip(range(len(simulator.alphas)), simulator.alphas):\n",
    "        p = game.transition_probability_matrix(t, alpha, policy,\n",
    "                                                DiscretizedGraphonMeanField(game.agent_observation_space,\n",
    "                                                                            [ConstantFiniteMeanField(\n",
    "                                                                                game.agent_observation_space,\n",
    "                                                                                mu_alpha)\n",
    "                                                                            for mu_alpha in mu_alphas_curr],\n",
    "                                                                            simulator.alphas))\n",
    "        mu_alpha = np.matmul(mu_alphas_curr[idx], p)\n",
    "        mu_alphas_next.append(mu_alpha)\n",
    "\n",
    "    mu_alphas_curr = mu_alphas_next\n",
    "\n",
    "\"\"\" Reshape \"\"\"\n",
    "final_mus = [\n",
    "    [mus[t][alpha_idx] for t in range(game.time_steps)]\n",
    "    for alpha_idx in range(simulator.num_alphas)\n",
    "]\n",
    "\n",
    "info = {'mus': mus, 'alphas': simulator.alphas}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ray (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ray\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
